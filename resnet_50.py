import os

# --- 1. C·∫§U H√åNH H·ªÜ TH·ªêNG ---
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'

import tensorflow as tf
from tensorflow.keras import layers, models, Input
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.applications.resnet50 import preprocess_input
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
import matplotlib.pyplot as plt
import pickle

# --- 2. THI·∫æT L·∫¨P ƒê∆Ø·ªúNG D·∫™N ---
data_dir = r'F:\Desktop\Do_an_2\datasets\AugmentedAlzheimerDataset'
weights_dir = r'F:\Desktop\Do_an_2\weights'

# ƒê∆∞·ªùng d·∫´n file model t·ªët nh·∫•t
model_path_best = os.path.join(weights_dir, 'resnet50_finetuned_best.keras')

IMG_HEIGHT = 170
IMG_WIDTH = 170
TARGET_SIZE = 224 # K√≠ch th∆∞·ªõc t·ªëi ∆∞u cho ResNet
BATCH_SIZE = 16   # Gi·∫£m batch size v√¨ ResNet t·ªën VRAM h∆°n

if not os.path.exists(weights_dir):
    os.makedirs(weights_dir)

# --- 3. N·∫†P D·ªÆ LI·ªÜU ---
print("--- ƒêang n·∫°p d·ªØ li·ªáu ---")
train_ds = tf.keras.utils.image_dataset_from_directory(
    data_dir,
    validation_split=0.2,
    subset="training",
    seed=123,
    image_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=BATCH_SIZE)

val_ds = tf.keras.utils.image_dataset_from_directory(
    data_dir,
    validation_split=0.2,
    subset="validation",
    seed=123,
    image_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=BATCH_SIZE)

class_names = train_ds.class_names
num_classes = len(class_names)
print(f"C√°c l·ªõp: {class_names}")

# T·ªëi ∆∞u hi·ªáu nƒÉng n·∫°p d·ªØ li·ªáu
AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

# --- 4. X√ÇY D·ª∞NG MODEL (KI·∫æN TR√öC M·∫†NH M·∫º) ---
print("\n--- X√¢y d·ª±ng Model ResNet50 Fine-Tune ---")

# L·ªõp tƒÉng c∆∞·ªùng d·ªØ li·ªáu (Ch·ªâ ho·∫°t ƒë·ªông khi training)
data_augmentation = models.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.05), # Xoay r·∫•t nh·∫π (5%)
    layers.RandomZoom(0.05),
])

inputs = Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))

# B∆∞·ªõc 1: Augmentation
x = data_augmentation(inputs)

# B∆∞·ªõc 2: Resize l√™n 224x224 (ResNet th√≠ch size n√†y)
x = layers.Resizing(TARGET_SIZE, TARGET_SIZE)(x)

# B∆∞·ªõc 3: Preprocess chu·∫©n c·ªßa ResNet (quan tr·ªçng!)
# H√†m n√†y s·∫Ω chuy·ªÉn ƒë·ªïi pixel ph√π h·ª£p v·ªõi c√°ch ResNet ƒë∆∞·ª£c train tr√™n ImageNet
x = preprocess_input(x)

# B∆∞·ªõc 4: Base Model
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(TARGET_SIZE, TARGET_SIZE, 3))
base_model.trainable = False # Ban ƒë·∫ßu ƒë√≥ng bƒÉng

x = base_model(x, training=False) # training=False ƒë·ªÉ kh√≥a BatchNormalization layer

# B∆∞·ªõc 5: Classification Head (Ph·∫ßn ƒë·∫ßu ra m·ªõi)
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dense(512, activation='relu')(x)
x = layers.Dropout(0.5)(x) # Ch·ªëng overfitting
outputs = layers.Dense(num_classes, activation='softmax')(x)

model = models.Model(inputs, outputs, name="ResNet50_Alzheimer_FineTuned")

# --- 5. GIAI ƒêO·∫†N 1: WARM-UP (Hu·∫•n luy·ªán nh·∫π) ---
print("\nüî• GIAI ƒêO·∫†N 1: Train l·ªõp ƒë·∫ßu ra (Base ƒë√≥ng bƒÉng)")
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Train nhanh 10 epochs ƒë·ªÉ c√°c l·ªõp Dense h·ªçc ƒë∆∞·ª£c ch√∫t √≠t
history_warmup = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=10
)

# --- 6. GIAI ƒêO·∫†N 2: FINE-TUNING (Hu·∫•n luy·ªán s√¢u) ---
print("\n‚ùÑÔ∏è GIAI ƒêO·∫†N 2: Unfreeze & Fine-Tune (Quan tr·ªçng nh·∫•t)")

# M·ªü kh√≥a base model
base_model.trainable = True

# ResNet50 c√≥ kho·∫£ng 175 layers.
# Ta s·∫Ω ƒë√≥ng bƒÉng 140 l·ªõp ƒë·∫ßu (gi·ªØ l·∫°i kh·∫£ nƒÉng nh·∫≠n di·ªán c·∫°nh c∆° b·∫£n)
# Ch·ªâ train l·∫°i kho·∫£ng 30-40 l·ªõp cu·ªëi (nh·∫≠n di·ªán ƒë·∫∑c tr∆∞ng tr·ª´u t∆∞·ª£ng c·ªßa n√£o)
fine_tune_at = 140

for layer in base_model.layers[:fine_tune_at]:
    layer.trainable = False

print(f"-> ƒê√£ m·ªü kh√≥a t·ª´ layer {fine_tune_at} tr·ªü ƒëi.")

# QUAN TR·ªåNG: Compile l·∫°i v·ªõi Learning Rate C·ª∞C NH·ªé
# N·∫øu ƒë·ªÉ LR l·ªõn (nh∆∞ 0.001), n√≥ s·∫Ω ph√° v·ª° c√°c tr·ªçng s·ªë t·ªët s·∫µn c√≥.
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), 
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

callbacks = [
    ModelCheckpoint(model_path_best, save_best_only=True, monitor='val_accuracy', mode='max', verbose=1),
    EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True),
    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-7)
]

# Train ti·∫øp 20-30 epochs n·ªØa
total_epochs = 30
history_finetune = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=total_epochs,
    initial_epoch=history_warmup.epoch[-1], # Ti·∫øp n·ªëi s·ªë epoch c≈©
    callbacks=callbacks
)

# --- 7. V·∫º BI·ªÇU ƒê·ªí T·ªîNG H·ª¢P ---
print("\n--- V·∫Ω bi·ªÉu ƒë·ªì k·∫øt qu·∫£ ---")

# N·ªëi l·ªãch s·ª≠ hu·∫•n luy·ªán c·ªßa 2 giai ƒëo·∫°n
acc = history_warmup.history['accuracy'] + history_finetune.history['accuracy']
val_acc = history_warmup.history['val_accuracy'] + history_finetune.history['val_accuracy']
loss = history_warmup.history['loss'] + history_finetune.history['loss']
val_loss = history_warmup.history['val_loss'] + history_finetune.history['val_loss']

plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
# V·∫Ω v·∫°ch ngƒÉn c√°ch 2 giai ƒëo·∫°n
plt.plot([9, 9], plt.ylim(), label='B·∫Øt ƒë·∫ßu Fine Tuning', linestyle='--', color='green')
plt.legend(loc='lower right')
plt.title('Accuracy: Warmup + FineTuning')

plt.subplot(1, 2, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.plot([9, 9], plt.ylim(), label='B·∫Øt ƒë·∫ßu Fine Tuning', linestyle='--', color='green')
plt.legend(loc='upper right')
plt.title('Loss: Warmup + FineTuning')

plt.show()